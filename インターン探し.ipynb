{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URLを取得し、解析する #\n",
    "def get_and_bs_html(dic):\n",
    "    html = requests.get(dic[\"URL\"]+dic[\"Conditions\"])\n",
    "    if dic[\"Name\"] == \"zero-one\":\n",
    "        bs = BeautifulSoup(html.content,\"html.parser\")\n",
    "    else:\n",
    "        bs = BeautifulSoup(html.text,\"html.parser\")\n",
    "    return bs\n",
    "\n",
    "# 新着記事かどうかの判断基準を決める #\n",
    "def select_criterion(dic,df):\n",
    "    df_sorted_by_source = df[df[\"Source\"]==dic[\"Name\"]]\n",
    "    if (dic[\"Name\"] == \"Infra\") | (dic[\"Name\"] == \"Wantedly\"):\n",
    "        latest_date = list(df_sorted_by_source[-1:][\"Date\"].values)[0]\n",
    "        criterion = df_sorted_by_source[df_sorted_by_source[\"Date\"]==latest_date][:1][\"Title\"].values # latest title\n",
    "    else:\n",
    "        criterion = list(df_sorted_by_source[\"Title\"]) # titles_stored\n",
    "    return criterion, df_sorted_by_source\n",
    "\n",
    "# サイトによって異なるスクレイピング関数 #\n",
    "def search_new_article(dic,df):\n",
    "    # 前準備 #\n",
    "    columns = [\"Title\",\"URL\",\"Summary\",\"Source\",\"Date\"]\n",
    "    today = datetime.date.today().strftime(\"%Y/%m/%d\")\n",
    "    bs = get_and_bs_html(dic)\n",
    "    criterion,df_sorted_by_source = select_criterion(dic,df)\n",
    "    # 本番 #\n",
    "    if dic[\"Name\"] == \"キャリアバイト\": # キャリアバイト用\n",
    "        all_articles = bs.find_all(\"a\",{\"class\":\"tapArea informClick\"})\n",
    "        for article in all_articles:\n",
    "            avalibility = article.find(\"span\",{\"class\":\"newLabel\"})\n",
    "            if avalibility == None: # Newのラベルがなければbreak\n",
    "                break\n",
    "            else:\n",
    "                title = article.find(\"h3\",{\"class\":\"summaryList_ttl\"})\n",
    "                for e in title:\n",
    "                    if (e.string != \"NEW\") & (len(e.string)>1):\n",
    "                        title_ = e.string.strip(\"\\n \") # タイトルを特定\n",
    "                    else:\n",
    "                        continue\n",
    "                if title_ not in criterion: # 新着記事であれば\n",
    "                    url = dic[\"URL\"]+article.get(\"href\") # URLを特定\n",
    "                    # summaryを特定 #\n",
    "                    summary = article.find(\"div\",{\"class\":\"summaryList_lead\"}).string\n",
    "                    print(title_)\n",
    "                    print(url)\n",
    "                    print(summary)\n",
    "                    se = pd.Series([title,url,summary,\"キャリアバイト\",today],index=columns)\n",
    "                    df = df.append(se,ignore_index=True)\n",
    "                else:\n",
    "                    continue\n",
    "        # 新着記事がなければ #\n",
    "        if df[\"Source\"].value_counts()[dic[\"Name\"]] == df_sorted_by_source[\"Source\"].value_counts()[dic[\"Name\"]]:\n",
    "            print(dic[\"Name\"]+\"に新着求人はない\")\n",
    "            print(dic[\"URL\"]+dic[\"Conditions\"])\n",
    "    if dic[\"Name\"] == \"Infra\": # Infra用\n",
    "        all_articles = bs.find_all(\"div\",{\"class\":\"col s12 m6\"})\n",
    "        for article in all_articles:\n",
    "            title_and_url = article.find(\"h3\",{\"class\":\"intern-title\"})\n",
    "            title = title_and_url.find(\"a\").string.strip(\"\\n \") # タイトルを特定\n",
    "            # 新着記事がなければ #\n",
    "            if title == criterion:\n",
    "                break\n",
    "            # 新着記事であれば #\n",
    "            else:\n",
    "                # URLを取得 #\n",
    "                url = dic[\"URL\"]+title_and_url.find(\"a\").get(\"href\")\n",
    "                # summaryを取得 #\n",
    "                summary = article.find(\"td\",{\"class\":\"job-type\"}).string\n",
    "                print(title)\n",
    "                print(url)\n",
    "                print(summary)\n",
    "                se = pd.Series([title,url,summary,\"Infra\",today],index=columns)\n",
    "                df = df.append(se,ignore_index=True)\n",
    "        # 新着記事がなければ #\n",
    "        if df[\"Source\"].value_counts()[dic[\"Name\"]] == df_sorted_by_source[\"Source\"].value_counts()[dic[\"Name\"]]:\n",
    "            print(dic[\"Name\"]+\"に新着求人はない\")\n",
    "            print(dic[\"URL\"]+dic[\"Conditions\"])\n",
    "    if dic[\"Name\"] == \"zero-one\": # ゼロワン用\n",
    "        all_articles = bs.find_all(\"section\",{\"class\":\"l-common-joblistBox\"})\n",
    "        for article in all_articles:\n",
    "            avalibility = article.find(\"div\",{\"class\":\"l-common-joblist_fav_sp display-none_pc\"}).find(\"span\").string\n",
    "            if avalibility != \"お気に入り\": # 募集中でなければbreak\n",
    "                break\n",
    "            else:\n",
    "                title_and_url = article.find(\"div\",{\"class\":\"l-common-joblist_text\"})\n",
    "                # タイトルを取得 #\n",
    "                title = title_and_url.find(\"h4\").string.strip(\"\\n \")\n",
    "                # 募集中でありかつ新着であれば #\n",
    "                if title not in criterion:\n",
    "                    # URLを取得 #\n",
    "                    url = dic[\"URL\"] + title_and_url.find(\"a\").get(\"href\")\n",
    "                    summary_and_loc = article.find_all(\"dl\",{\"class\":\"display-none_sp\"})\n",
    "                    # summaryを取得 #\n",
    "                    summary = summary_and_loc[1].find(\"dd\").string.strip(\"\\n \")\n",
    "                    print(title)\n",
    "                    print(url)\n",
    "                    print(summary)\n",
    "                    se = pd.Series([title,url,summary,\"zero-one\",today],index=columns)\n",
    "                    df = df.append(se,ignore_index=True)\n",
    "                else:\n",
    "                    continue\n",
    "        # 新着記事がなければ #\n",
    "        if df[\"Source\"].value_counts()[dic[\"Name\"]] == df_sorted_by_source[\"Source\"].value_counts()[dic[\"Name\"]]:\n",
    "            print(dic[\"Name\"]+\"に新着求人はない\")\n",
    "            print(dic[\"URL\"]+dic[\"Conditions\"])\n",
    "    if dic[\"Name\"] == \"Wantedly\": # Wantedly用\n",
    "        all_articles = bs.find_all(\"article\",{\"class\":\"projects-index-single\"})\n",
    "        for article in all_articles:\n",
    "            title = article.find(\"h1\",{\"class\":\"project-title\"}).find(\"a\").string # タイトルを特定\n",
    "            # 新着記事がなければ #\n",
    "            if title == criterion:\n",
    "                break\n",
    "            # 新着記事であれば #\n",
    "            else:\n",
    "                # URLを取得 #\n",
    "                url = dic[\"URL\"]+article.find(\"h1\",{\"class\":\"project-title\"}).find(\"a\").get(\"href\")\n",
    "                # summaryを取得 #\n",
    "                summary = article.find(\"p\",{\"class\":\"project-excerpt\"}).string.strip(\"\\n \")\n",
    "                print(title)\n",
    "                print(url)\n",
    "                print(summary)\n",
    "                se = pd.Series([title,url,summary,\"Wantedly\",today],index=columns)\n",
    "                df = df.append(se,ignore_index=True)\n",
    "        # 新着記事がなければ #\n",
    "        if df[\"Source\"].value_counts()[dic[\"Name\"]] == df_sorted_by_source[\"Source\"].value_counts()[dic[\"Name\"]]:\n",
    "            print(dic[\"Name\"]+\"に新着求人はない\")\n",
    "            print(dic[\"URL\"]+dic[\"Conditions\"])\n",
    "    return df\n",
    "# サイトおよび条件の辞書 #\n",
    "dic_career = {\n",
    "    \"Name\":\"キャリアバイト\",\n",
    "    \"URL\":\"https://careerbaito.com\",\n",
    "    \"Conditions\":\"/search/38/305/316\"\n",
    "}\n",
    "dic_infra = {\n",
    "    \"Name\":\"Infra\",\n",
    "    \"URL\":\"https://www.in-fra.jp/\",\n",
    "    \"Conditions\":\"long-internships?occupation%5B%5D=1&occupation%5B%5D=7&area%5B%5D=1&area%5B%5D=4&area%5B%5D=5&area%5B%5D=2&industry%5B%5D=1&industry%5B%5D=4&industry%5B%5D=11&order=recent\"\n",
    "}\n",
    "dic_zeroone = {\n",
    "    \"Name\":\"zero-one\",\n",
    "    \"URL\":\"https://01intern.com\",\n",
    "    \"Conditions\":\"/job/list.html?areas=13&jobTypes=6&jobTypes=2&businessTypes=1&businessTypes=4&businessTypes=8\"\n",
    "}\n",
    "dic_wantedly = {\n",
    "    \"Name\":\"Wantedly\",\n",
    "    \"URL\":\"https://www.wantedly.com/\",\n",
    "    \"Conditions\":\"/projects?type=recent&page=1&occupation_types%5B%5D=jp__engineering&hiring_types%5B%5D=internship&fields%5B%5D=jp__data_scientist&locations%5B%5D=tokyo\"\n",
    "}\n",
    "dic_list = [dic_career,dic_infra,dic_zeroone,dic_wantedly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "csv_path = os.path.join(\"datasets\",\"Internship.csv\")\n",
    "df = pd.read_csv(csv_path,index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "for target in dic_list:\n",
    "    df = search_new_article(target,df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(csv_path,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
