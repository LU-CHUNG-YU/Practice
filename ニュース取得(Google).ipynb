{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## (Clear)指定のキーワードが含まれるニュースのタイトルとURLを抽出し、保存する ##\n",
        "# (Clear)「複数地域」「複数時間」の google news 検索結果ページで行う #\n",
        "# (Clear)発布時間で並び替える #\n",
        "#（Clear）指定のソース・日付の記事のみ表示 #\n",
        "# (Clear)記事の概要にアクセスし、特定の記述が含まれるもののみ抽出 #\n",
        "# 抽出された結果の横に「チェックボックス」をいれ、チェックされたもののみ再表示、保存する #"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib\n",
        "import requests\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 検索条件による検索結果ページのURLを確定し、辞書に格納する ##\n",
        "google_news_url = \"https://news.google.com\"\n",
        "dic_en = {\n",
        "    \"key_word\":\"self driving\",\n",
        "    \"time\":\"1h\", # Options:1h,1d,7d,1y,any\n",
        "    \"language\":\"ENG\",\n",
        "    \"URL\":\"&hl=en-US&gl=US&ceid=US%3Aen\"\n",
        "}\n",
        "dic_jp = {\n",
        "    \"key_word\":\"野球\",\n",
        "    \"time\":\"1d\",\n",
        "    \"language\":\"JPN\",\n",
        "    \"URL\":\"&hl=ja&gl=JP&ceid=JP%3Aja\"\n",
        "}\n",
        "dic_list = [dic_en,dic_jp]\n",
        "\n",
        "url_dic = {}\n",
        "for dic in dic_list:\n",
        "    constraints = dic[\"key_word\"] if dic[\"time\"]==\"any\" else dic[\"key_word\"] + \" when:\" + dic[\"time\"]\n",
        "    print(constraints)\n",
        "    quote = urllib.parse.quote(constraints) # constraintsをURL中の形式に変換する\n",
        "    URL = google_news_url+\"/search?q=\"+quote+dic[\"URL\"]\n",
        "    print(URL)\n",
        "    url_dic[dic[\"language\"]] = URL\n",
        "url_dic\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 検査結果ページにある記事のタイトル・URL・概要・ソース・日付をスクレイプングする ##\n",
        "def scrapping_news(language): # Options:\"JPN\",\"ENG\"\n",
        "    html = requests.get(url_dic[language]).text # htmlの取得\n",
        "    bs = BeautifulSoup(html, 'html.parser') # htmlの解析\n",
        "    span = bs.find_all(\"div\",{\"class\":\"xrnccd\"}) # 記事の情報が格納される html 文を見つける #\n",
        "    # 各記事の html 文からさらにタイトル・URL・ソースが格納される部分を抽出 #\n",
        "    list_ = []\n",
        "    columns=[\"Date\",\"Title\",\"Abstract\",\"URL\",\"Source\"]\n",
        "    df = pd.DataFrame(columns=columns)\n",
        "    for tag in span:  # tagは個々記事のhtml文（タグ＝div,class=xrnccdの部分）を表し、それぞれに対して以下の処理を行う\n",
        "        title_and_url = tag.find(\"a\",{\"class\":\"DY5T1d\"}) # タイトル・URLが格納される文\n",
        "        source = tag.find(\"a\",{\"class\":\"wEwyrc AVN2gc uQIVzc Sksgp\"}) # ソースが格納される文\n",
        "        time = tag.find(\"time\",{\"class\":\"WW6dff uQIVzc Sksgp\"}) # 日付が格納される文\n",
        "        abstract = tag.find(\"span\",{\"class\":\"xBbh9\"}) # 概要が格納される文\n",
        "        title = title_and_url.string # タイトルは文字部分を指定することで取得できる\n",
        "        if title not in list_:\n",
        "            list_.append(title)\n",
        "            print(title)\n",
        "            URL = google_news_url+title_and_url.get(\"href\") # URLは属性：hrefに格納される\n",
        "            print(URL)\n",
        "            abstract = abstract.string # 概要は文字部分を指定することで取得できる\n",
        "            print(abstract)\n",
        "            source = source.string # ソースも文字部分を指定することで取得できる\n",
        "            print(\"Source:\"+source)\n",
        "            datetime_ = time.get(\"datetime\")[0:10].replace(\"-\",\"/\") # 日付は属性：datetimeに格納される\n",
        "            print(\"Published Date:\"+datetime_)\n",
        "            se = pd.Series([datetime_,title,abstract,URL,source],columns)\n",
        "            df = df.append(se,columns)\n",
        "    return df\n",
        "table_en = scrapping_news(\"ENG\")\n",
        "table_jp = scrapping_news(\"JPN\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_en\n",
        "table_jp\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## スクレイプングした記事を日付の昇順・降順で並び替える ##\n",
        "def sort_table_by_date(table,ascend=True):\n",
        "    Date_str = table.loc[:,\"Date\"] # str形式の日付を保存する\n",
        "    columns = table.columns # columnsを保存する（後でDataFrameを作る際に便利）\n",
        "\n",
        "    list_ = []\n",
        "    for i in table.values:\n",
        "        list_.append(int(i[0].replace(\"/\",\"\"))) # sortするために、日付を数値形式に変え、リストに格納する\n",
        "    table.loc[:,\"Date\"] = list_ # リストを使い、元tableの日付を数値化されたもので置き換える\n",
        "    #（後で使うnp.argsortはnumpy配列のみに有効だから、新tableは作らなければならない）#\n",
        "\n",
        "    # 新tableの日付の部分を抽出し並び替え、その結果のインデックスを取得する\n",
        "    if ascend:\n",
        "        index_sorted = np.argsort(table.values[:,0],axis=0)\n",
        "    else:\n",
        "        index_sorted = np.argsort(table.values[:,0],axis=0)[::-1]\n",
        "\n",
        "    table.loc[:,\"Date\"] = Date_str # str形式の日付を使い元tableに還元する\n",
        "    # 元tableのvalues部分を新インデックスに従い並び替え、それに基づき最終のsortしたtableを作る\n",
        "    table_sorted = pd.DataFrame(table.values[index_sorted],columns=columns)\n",
        "    return table_sorted"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_jp_sorted = sort_table_by_date(table_jp,False)\n",
        "table_jp_sorted\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 指定のソース・日付の記事のみ表示させる ##\n",
        "dic_condition_jp = {\n",
        "    \"condition1\":None,\n",
        "    \"condition2\":\"Yahoo!ニュース\"\n",
        "}\n",
        "def pick_up_by_source_or_time(table,dic):\n",
        "    df = pd.DataFrame(columns=table.columns)\n",
        "    columns = list(table.columns)\n",
        "    # 適用される条件を確認する（Date,Source共に制限するか、一方のみか）#\n",
        "    if (dic[\"condition1\"]!=None) & (dic[\"condition2\"]==None): # Dateのみ\n",
        "        for i in table.values:\n",
        "            if i[0]==dic[\"condition1\"]: # Dateで判断\n",
        "                    print(i[1])\n",
        "                    print(i[2])\n",
        "                    print(i[3])\n",
        "                    print(\"Source:\"+i[4])\n",
        "                    print(\"Published Date:\"+i[0])\n",
        "                    se = pd.Series([i[0],i[1],i[2],i[3],i[4]],columns)\n",
        "                    df = df.append(se,columns)\n",
        "            else:\n",
        "                continue\n",
        "    elif (dic[\"condition1\"]==None) & (dic[\"condition2\"]!=None): # Sourceのみ\n",
        "        for i in table.values:\n",
        "            if i[4]==dic[\"condition2\"]: # Sourceで判断\n",
        "                    print(i[1])\n",
        "                    print(i[2])\n",
        "                    print(i[3])\n",
        "                    print(\"Source:\"+i[4])\n",
        "                    print(\"Published Date:\"+i[0])\n",
        "                    se = pd.Series([i[0],i[1],i[2],i[3],i[4]],columns)\n",
        "                    df = df.append(se,columns)\n",
        "            else:\n",
        "                continue\n",
        "    elif (dic[\"condition1\"]!=None) & (dic[\"condition2\"]!=None): # 両方\n",
        "        for i in table.values:\n",
        "            if i[4]==dic[\"condition2\"] & i[0]==dic[\"condition1\"]: # 両方同時判断\n",
        "                    print(i[1])\n",
        "                    print(i[2])\n",
        "                    print(i[3])\n",
        "                    print(\"Source:\"+i[4])\n",
        "                    print(\"Published Date:\"+i[0])\n",
        "                    se = pd.Series([i[0],i[1],i[2],i[3],i[4]],columns)\n",
        "                    df = df.append(se,columns)\n",
        "            else:\n",
        "                continue\n",
        "    return df\n",
        "table_jp_picked_by_source_or_time = pick_up_by_source_or_time(table=table_jp,dic=dic_condition_jp)\n",
        "table_jp_picked_source_or_time\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 記事の概要にアクセスし、特定の記述が含まれるもののみ抽出 ##\n",
        "key_words_list_jp = [\"緊急事態宣言\",\"プロ\",\"コロナ\",\"ホームラン\"]\n",
        "\n",
        "def pick_up_by_contents(table,key_words_list):\n",
        "    list_ = []\n",
        "    df = pd.DataFrame(columns=table.columns)\n",
        "    columns = list(table.columns)\n",
        "    for word in key_words_list:\n",
        "        for i in table.values:\n",
        "            if (word in i[2]) & (i[1] not in list_):\n",
        "                list_.append(i[1])\n",
        "                print(i[1])\n",
        "                print(i[2])\n",
        "                print(i[3])\n",
        "                print(\"Source:\"+i[4])\n",
        "                print(\"Published Date:\"+i[0])\n",
        "                se = pd.Series([i[0],i[1],i[2],i[3],i[4]],columns)\n",
        "                df = df.append(se,columns)\n",
        "            else:\n",
        "                continue\n",
        "    return df\n",
        "table_jp_picked_by_contents = pick_up_by_contents(table=table_jp,key_words_list=key_words_list_jp)\n",
        "table_jp_picked_by_contents\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}